{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686811bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paper 6 Reproduction Script\n",
    "\"\"\"\n",
    "Title: \"Predicting academic performance of students from VLE big data using deep learning models\"\n",
    "Authors: Md Shoaib Ahmed, Shazia Sadiq, Ashad Kabir, Shazia W. Sadiq, Rolf A. Schwitter (2021)\n",
    "\n",
    "Purpose:\n",
    "End-to-end entry-point script for academic performance prediction:\n",
    "    - Data loading and preprocessing\n",
    "    - Feature engineering\n",
    "    - Train/test split\n",
    "    - Deep learning model training and evaluation\n",
    "    - Save trained model weights\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a530639f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Imports + RNG seeds\n",
    "import os, json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import (classification_report, roc_auc_score,\n",
    "                             accuracy_score, precision_score, recall_score, log_loss)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel(\"ERROR\")\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9e176b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Load data\n",
    "si   = pd.read_csv('studentInfo.csv')\n",
    "svle = pd.read_csv('studentVle.csv')\n",
    "vle  = pd.read_csv('vle.csv')\n",
    "sa   = pd.read_csv('studentAssessment.csv')\n",
    "ass  = pd.read_csv('assessments.csv')\n",
    "crs  = pd.read_csv('courses.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10242f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Feature engineering (clicks, assessments, master table)\n",
    "svle = svle.merge(vle[['id_site','activity_type']], on='id_site', how='left')\n",
    "\n",
    "clicks = (\n",
    "    svle.groupby(['id_student','code_module','code_presentation'])['sum_click']\n",
    "        .sum()\n",
    "        .reset_index(name='total_clicks')\n",
    ")\n",
    "\n",
    "sa_aug = sa.merge(ass, on='id_assessment', how='left')\n",
    "ass_agg = (\n",
    "    sa_aug.groupby(['id_student','code_module','code_presentation'])\n",
    "          .agg(avg_score=('score','mean'),\n",
    "               n_submissions=('id_assessment','size'))\n",
    "          .reset_index()\n",
    ")\n",
    "\n",
    "data = (\n",
    "    si.merge(clicks,  on=['id_student','code_module','code_presentation'], how='left')\n",
    "      .merge(ass_agg, on=['id_student','code_module','code_presentation'], how='left')\n",
    ")\n",
    "data = data.dropna(subset=['final_result'])\n",
    "\n",
    "def task_pass_fail(df):\n",
    "    subset = df[df.final_result.isin(['Pass','Distinction','Fail'])].copy()\n",
    "    subset['label'] = (subset.final_result == 'Fail').astype(int)\n",
    "    return subset\n",
    "\n",
    "def task_withdrawn_pass(df):\n",
    "    subset = df[df.final_result.isin(['Pass','Distinction','Withdrawn'])].copy()\n",
    "    subset['label'] = (subset.final_result == 'Withdrawn').astype(int)\n",
    "    return subset\n",
    "\n",
    "def task_dist_pass(df):\n",
    "    subset = df[df.final_result.isin(['Pass','Distinction'])].copy()\n",
    "    subset['label'] = (subset.final_result == 'Distinction').astype(int)\n",
    "    return subset\n",
    "\n",
    "def task_dist_fail(df):\n",
    "    subset = df[df.final_result.isin(['Distinction','Fail'])].copy()\n",
    "    subset['label'] = (subset.final_result == 'Distinction').astype(int)\n",
    "    return subset\n",
    "\n",
    "tasks = {\n",
    "    'Pass_vs_Fail'        : task_pass_fail(data),\n",
    "    'Withdrawn_vs_Pass'   : task_withdrawn_pass(data),\n",
    "    'Distinction_vs_Pass' : task_dist_pass(data),\n",
    "    'Distinction_vs_Fail' : task_dist_fail(data)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba8b190",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Pipeline matrices\n",
    "imputer = SimpleImputer(strategy='constant', fill_value=0)\n",
    "scaler  = MinMaxScaler()\n",
    "task_matrices = {}\n",
    "\n",
    "for name, df in tasks.items():\n",
    "    y = df['label'].values\n",
    "    num_cols = df.select_dtypes(include='number').columns.difference(['id_student','label'])\n",
    "    X_num = imputer.fit_transform(df[num_cols])\n",
    "    X_num = scaler.fit_transform(X_num)\n",
    "    cat_cols = df.select_dtypes(include='object').columns.difference(['final_result'])\n",
    "    X_cat = pd.get_dummies(df[cat_cols], dummy_na=True, drop_first=True)\n",
    "    X_full = np.hstack([X_num, X_cat.values])\n",
    "    if X_full.shape[1] > 30:\n",
    "        svd = TruncatedSVD(n_components=30, random_state=SEED)\n",
    "        X_red = svd.fit_transform(X_full)\n",
    "        X_df = pd.DataFrame(X_red, index=df.index, columns=[f'F{i+1}' for i in range(30)])\n",
    "    else:\n",
    "        X_df = pd.DataFrame(X_full, index=df.index,\n",
    "                            columns=[f'X{i+1}' for i in range(X_full.shape[1])])\n",
    "    task_matrices[name] = X_df.assign(label=y)\n",
    "\n",
    "for k, m in task_matrices.items():\n",
    "    print(f'{k:22s} -> {m.shape}')\n",
    "\n",
    "# Step 4.1: Static features (54 -> SVD 30)\n",
    "static = data[['id_student','code_module','code_presentation',\n",
    "               'highest_education','imd_band','age_band',\n",
    "               'num_of_prev_attempts','studied_credits',\n",
    "               'disability','gender','region']]\n",
    "\n",
    "svle_split = svle.copy()\n",
    "\n",
    "def click_totals(df, mask, prefix):\n",
    "    sub = df[mask]\n",
    "    tot  = (sub.groupby(['id_student','code_module','code_presentation'])['sum_click']\n",
    "              .sum().rename(f'{prefix}T_Clicks'))\n",
    "    acts = (sub.pivot_table(index=['id_student','code_module','code_presentation'],\n",
    "                            columns='activity_type', values='sum_click', aggfunc='sum')\n",
    "              .add_prefix(prefix))\n",
    "    return pd.concat([tot, acts], axis=1).reset_index()\n",
    "\n",
    "before = click_totals(svle_split, svle_split['date'] < 0,  'BC_')\n",
    "after  = click_totals(svle_split, svle_split['date'] >= 0, 'AC_')\n",
    "\n",
    "ass_flags = (\n",
    "    sa_aug.assign(late = sa_aug['date_submitted'] > sa_aug['date'])\n",
    "          .groupby(['id_student','code_module','code_presentation'])\n",
    "          .agg(ModuleAsigns=('id_assessment','nunique'),\n",
    "               LateAsignsSub=('late','sum'))\n",
    "          .reset_index()\n",
    ")\n",
    "\n",
    "sa_dead = sa_aug[['id_student','code_module','code_presentation',\n",
    "                  'id_assessment','date']].rename(columns={'date':'deadline_day'})\n",
    "\n",
    "click_base = svle[['id_student','code_module','code_presentation',\n",
    "                   'date','sum_click']].rename(columns={'date':'click_day'})\n",
    "\n",
    "def clicks_window(offset, col_name):\n",
    "    merged = sa_dead.merge(click_base, on=['id_student','code_module','code_presentation'], how='left')\n",
    "    mask   = merged['click_day'] == merged['deadline_day'] + offset\n",
    "    out    = (merged.loc[mask]\n",
    "                    .groupby(['id_student','code_module','code_presentation'])['sum_click']\n",
    "                    .sum()\n",
    "                    .rename(col_name)\n",
    "                    .reset_index())\n",
    "    return out\n",
    "\n",
    "F25 = clicks_window(-1,  'PreA_1')\n",
    "F27 = clicks_window( 0,  'OnAsClicks')\n",
    "F30 = clicks_window(10,  'PostA_10')\n",
    "\n",
    "keys = ['id_student','code_module','code_presentation']\n",
    "static_full = (static\n",
    "               .merge(before, on=keys, how='left')\n",
    "               .merge(after,  on=keys, how='left')\n",
    "               .merge(ass_flags, on=keys, how='left')\n",
    "               .merge(clicks.rename(columns={'total_clicks':'Tc_Activity'}), on=keys, how='left')\n",
    "               .merge(F25, on=keys, how='left')\n",
    "               .merge(F27, on=keys, how='left')\n",
    "               .merge(F30, on=keys, how='left')\n",
    "               .fillna(0))\n",
    "\n",
    "X = static_full.drop(columns=keys)\n",
    "X = pd.get_dummies(X, dummy_na=True, drop_first=True)\n",
    "X = SimpleImputer(strategy='constant', fill_value=0).fit_transform(X)\n",
    "X = MinMaxScaler().fit_transform(X)\n",
    "static_SVD30 = TruncatedSVD(n_components=30, random_state=SEED).fit_transform(X)\n",
    "print(\"Static feature matrix:\", static_SVD30.shape)\n",
    "\n",
    "# Step 4.2: Temporal features (quartiles Ã— 20 acts)\n",
    "master_idx = data[keys].drop_duplicates().set_index(keys)\n",
    "activity_types = sorted(vle['activity_type'].dropna().unique().tolist())\n",
    "\n",
    "len_cols = [c for c in crs.columns if 'length' in c.lower()]\n",
    "if len_cols:\n",
    "    len_col = len_cols[0]\n",
    "    pres_len = crs[['code_module','code_presentation', len_col]].rename(columns={len_col: 'length'})\n",
    "else:\n",
    "    pres_len = (svle.groupby(['code_module','code_presentation'])['date']\n",
    "                      .max().reset_index(name='length'))\n",
    "\n",
    "svle_len = svle.merge(pres_len, on=['code_module','code_presentation'], how='left', validate='m:1')\n",
    "\n",
    "def make_quartile(q_num):\n",
    "    frac  = q_num / 4.0\n",
    "    label = f'Q{q_num}'\n",
    "    sub = svle_len[svle_len['date'] <= svle_len['length']*frac]\n",
    "    clicks_q = (sub.groupby(keys + ['activity_type'])['sum_click']\n",
    "                   .sum().unstack(fill_value=0)\n",
    "                   .reindex(columns=activity_types, fill_value=0)\n",
    "                   .add_prefix(f'{label}_'))\n",
    "    full = master_idx.join(clicks_q, how='left').fillna(0).reset_index()\n",
    "    return full\n",
    "\n",
    "temporal_frames = [make_quartile(q) for q in range(1, 5)]\n",
    "for lbl, df_ in zip(['Q1','Q2','Q3','Q4'], temporal_frames):\n",
    "    print(f'{lbl} shape:', df_.shape)\n",
    "\n",
    "assert static_full.drop(columns=keys).shape[1] == 54\n",
    "for q in temporal_frames:\n",
    "    assert q.filter(regex='^Q[1-4]_').shape[1] == 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d624cd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Deep ANN for four tasks\n",
    "def activity_only(df, prefix):\n",
    "    df_idx = df.set_index(keys)\n",
    "    acts = df_idx.filter(regex=f'^{prefix}')\n",
    "    return acts.astype('float32')\n",
    "\n",
    "X_static = (pd.DataFrame(static_SVD30,\n",
    "                         columns=[f'S{i+1}' for i in range(30)],\n",
    "                         index=static_full.set_index(keys).index)\n",
    "            .astype('float32'))\n",
    "X_q4 = activity_only(temporal_frames[3], 'Q4_')\n",
    "X_q2 = activity_only(temporal_frames[1], 'Q2_')\n",
    "X_hybrid = X_static.join(X_q2, how='left').fillna(0.0)\n",
    "\n",
    "feature_sets = {'STATIC-30': X_static, 'TEMP-Q4': X_q4, 'HYBRID-50': X_hybrid}\n",
    "\n",
    "def safe_name(txt):\n",
    "    return txt.replace(\" \", \"\").replace(\"|\", \"__\")\n",
    "\n",
    "def paths(task, fset):\n",
    "    base = safe_name(f\"{task}__{fset}\")\n",
    "    return f\"logs/{base}.csv\", f\"weights/{base}.h5\"\n",
    "\n",
    "def build_model(input_dim):\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(input_dim,)),\n",
    "        tf.keras.layers.Dense(50, activation='relu'),\n",
    "        tf.keras.layers.Dense(20, activation='relu'),\n",
    "        tf.keras.layers.Dense(10, activation='relu'),\n",
    "        tf.keras.layers.Dense(1,  activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "def resume_or_fit(model, Xtr, ytr, Xval, yval,\n",
    "                  csv_path, w_path,\n",
    "                  epochs=100, batch_size=64, class_wt=None):\n",
    "    initial_epoch = 0\n",
    "    if os.path.exists(csv_path):\n",
    "        initial_epoch = pd.read_csv(csv_path)['epoch'].max() + 1\n",
    "        if initial_epoch >= epochs:\n",
    "            print(\"âœ”  Training already complete â€“ skipping\")\n",
    "    else:\n",
    "        os.makedirs(os.path.dirname(csv_path), exist_ok=True)\n",
    "        os.makedirs(os.path.dirname(w_path),  exist_ok=True)\n",
    "\n",
    "    csv_cb = tf.keras.callbacks.CSVLogger(csv_path, append=os.path.exists(csv_path))\n",
    "    es_cb  = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy',\n",
    "                           tf.keras.metrics.Precision(name='precision'),\n",
    "                           tf.keras.metrics.Recall(name='recall'),\n",
    "                           tf.keras.metrics.AUC(name='auc')])\n",
    "\n",
    "    if os.path.exists(w_path):\n",
    "        model.load_weights(w_path)\n",
    "\n",
    "    if initial_epoch < epochs:\n",
    "        model.fit(Xtr, ytr,\n",
    "                  validation_data=(Xval, yval),\n",
    "                  epochs=epochs,\n",
    "                  batch_size=batch_size,\n",
    "                  class_weight=class_wt,\n",
    "                  callbacks=[csv_cb, es_cb],\n",
    "                  initial_epoch=initial_epoch,\n",
    "                  verbose=2)\n",
    "        model.save_weights(w_path)\n",
    "\n",
    "    return model\n",
    "\n",
    "results = defaultdict(dict)\n",
    "\n",
    "for task_name, task_df in tasks.items():\n",
    "    y = task_df['label'].astype('float32').values\n",
    "    mi = pd.MultiIndex.from_frame(task_df[keys])\n",
    "\n",
    "    for fset_name, Xmat in feature_sets.items():\n",
    "        X = Xmat.loc[mi].values.astype('float32')\n",
    "        X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.30, stratify=y, random_state=SEED)\n",
    "        cw = compute_class_weight('balanced', classes=[0, 1], y=y_tr)\n",
    "        class_wt = {0: cw[0], 1: cw[1]}\n",
    "\n",
    "        ann = build_model(X.shape[1])\n",
    "        csv_path, w_path = paths(task_name, fset_name)\n",
    "        ann = resume_or_fit(ann, X_tr, y_tr, X_te, y_te,\n",
    "                            csv_path, w_path, epochs=100, batch_size=64, class_wt=class_wt)\n",
    "\n",
    "        eval_vals = ann.evaluate(X_te, y_te, verbose=0)\n",
    "        raw = dict(zip(ann.metrics_names, eval_vals))\n",
    "        scores = {}\n",
    "        for k, v in raw.items():\n",
    "            base = k.split('_')[0]\n",
    "            scores[base] = float(v)\n",
    "        for k in ['accuracy', 'precision', 'recall', 'auc']:\n",
    "            scores.setdefault(k, np.nan)\n",
    "\n",
    "        results[task_name][fset_name] = scores\n",
    "\n",
    "        print(f\"{task_name:<18} | {fset_name:<9} | \"\n",
    "              f\"Acc {scores['accuracy']:.4f}  \"\n",
    "              f\"Prec {scores['precision']:.4f}  \"\n",
    "              f\"Rec {scores['recall']:.4f}  \"\n",
    "              f\"AUC {scores['auc']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34d66ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Baseline models\n",
    "baselines = {\n",
    "    'LogisticRegression': LogisticRegression(solver='liblinear', class_weight='balanced', random_state=SEED),\n",
    "    'SVM-RBF'           : SVC(kernel='rbf', probability=True, class_weight='balanced', random_state=SEED)\n",
    "}\n",
    "\n",
    "print(\"\\n=== Baseline results ===\")\n",
    "for task_name, task_df in tasks.items():\n",
    "    y = task_df['label'].values\n",
    "    idx = pd.MultiIndex.from_frame(task_df[keys])\n",
    "    for fset_name, Xmat in feature_sets.items():\n",
    "        X = Xmat.loc[idx].values\n",
    "        X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.30, stratify=y, random_state=SEED)\n",
    "        for model_name, model in baselines.items():\n",
    "            model.fit(X_tr, y_tr)\n",
    "            y_pred  = model.predict(X_te)\n",
    "            y_proba = model.predict_proba(X_te)[:,1]\n",
    "            acc = accuracy_score(y_te, y_pred)\n",
    "            prec = precision_score(y_te, y_pred, zero_division=0)\n",
    "            rec  = recall_score(y_te, y_pred, zero_division=0)\n",
    "            auc  = roc_auc_score(y_te, y_proba)\n",
    "            print(f\"{task_name:18s} | {fset_name:9s} | {model_name:18s} | \"\n",
    "                  f\"Acc {acc:.4f}  Prec {prec:.4f}  Rec {rec:.4f}  AUC {auc:.4f}\")\n",
    "\n",
    "# Step 6.1: Baseline repeated splits (static features)\n",
    "def eval_baseline(X, y, classifier, n_splits=10, test_size=0.3, random_state=SEED):\n",
    "    sss = StratifiedShuffleSplit(n_splits=n_splits, test_size=test_size, random_state=random_state)\n",
    "    accs, precs, recs, losses = [], [], [], []\n",
    "    for train_idx, test_idx in sss.split(X, y):\n",
    "        Xtr, Xte = X[train_idx], X[test_idx]\n",
    "        ytr, yte = y[train_idx], y[test_idx]\n",
    "        clf = classifier.fit(Xtr, ytr)\n",
    "        proba = clf.predict_proba(Xte)[:,1]\n",
    "        preds = clf.predict(Xte)\n",
    "        accs.append(accuracy_score(yte, preds))\n",
    "        precs.append(precision_score(yte, preds))\n",
    "        recs.append(recall_score(yte, preds))\n",
    "        losses.append(log_loss(yte, proba))\n",
    "    return {'accuracy': np.mean(accs)*100, 'loss': np.mean(losses),\n",
    "            'precision': np.mean(precs), 'recall': np.mean(recs)}\n",
    "\n",
    "lr  = LogisticRegression(penalty='l2', C=1.0, solver='liblinear', random_state=SEED)\n",
    "svm = SVC(kernel='rbf', C=1.0, gamma='scale', probability=True, random_state=SEED)\n",
    "\n",
    "results_tbl = []\n",
    "for task_name, df_ in tasks.items():\n",
    "    mi = pd.MultiIndex.from_frame(df_[['id_student','code_module','code_presentation']])\n",
    "    Xs = X_static.loc[mi].values\n",
    "    yy = df_['label'].values\n",
    "    for name, clf in [('LR', lr), ('SVM-RBF', svm)]:\n",
    "        scores = eval_baseline(Xs, yy, clf, n_splits=10)\n",
    "        results_tbl.append({\n",
    "            'Category': task_name.replace('_','/'),\n",
    "            'Technique': name,\n",
    "            'Accuracy (%)': scores['accuracy'],\n",
    "            'Loss': scores['loss'],\n",
    "            'Precision': scores['precision'],\n",
    "            'Recall': scores['recall']\n",
    "        })\n",
    "\n",
    "baseline_df = pd.DataFrame(results_tbl)\n",
    "print(baseline_df.pivot(index='Category', columns='Technique'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540e2d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Early prediction (quarterly)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "quartile_names = ['Q1','Q1-2','Q1-3','Q1-4']\n",
    "cum_frames = temporal_frames\n",
    "\n",
    "def get_activity_matrix(df, task_df):\n",
    "    idx = pd.MultiIndex.from_frame(task_df[['id_student','code_module','code_presentation']])\n",
    "    Xq  = df.set_index(['id_student','code_module','code_presentation'])\n",
    "    return Xq.loc[idx].astype('float32').values\n",
    "\n",
    "ep_results = {t:{'acc':[], 'loss':[]} for t in tasks}\n",
    "\n",
    "for task_name, task_df in tasks.items():\n",
    "    y = task_df['label'].astype('float32').values\n",
    "    for q_df in cum_frames:\n",
    "        X = get_activity_matrix(q_df, task_df)\n",
    "        Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.30, stratify=y, random_state=SEED)\n",
    "        cw = compute_class_weight('balanced', classes=[0,1], y=ytr)\n",
    "        class_wt = {0: cw[0], 1: cw[1]}\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Input(shape=(Xtr.shape[1],)),\n",
    "            tf.keras.layers.Dense(50, activation='relu'),\n",
    "            tf.keras.layers.Dense(20, activation='relu'),\n",
    "            tf.keras.layers.Dense(10, activation='relu'),\n",
    "            tf.keras.layers.Dense(1,  activation='sigmoid')\n",
    "        ])\n",
    "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "        model.fit(Xtr, ytr, validation_split=0.2, epochs=100, batch_size=64,\n",
    "                  class_weight=class_wt, callbacks=[es], verbose=0)\n",
    "        loss, acc = model.evaluate(Xte, yte, verbose=0)\n",
    "        ep_results[task_name]['acc'].append(acc)\n",
    "        ep_results[task_name]['loss'].append(loss)\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.subplot(1,2,1)\n",
    "for t, metrics in ep_results.items():\n",
    "    plt.plot(quartile_names, metrics['acc'], marker='o', label=t)\n",
    "plt.title('Quarterly Test Accuracy')\n",
    "plt.xlabel('Cumulative Quartile')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.xticks(rotation=15)\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "for t, metrics in ep_results.items():\n",
    "    plt.plot(quartile_names, metrics['loss'], marker='o', label=t)\n",
    "plt.title('Quarterly Test Loss')\n",
    "plt.xlabel('Cumulative Quartile')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.xticks(rotation=15)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
