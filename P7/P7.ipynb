{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7facbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# P7 Reproduction Script \n",
    "\"\"\"\n",
    "Title: \"Joint RNN Models for Early Prediction of Student Performance in Online Learning\"\n",
    "Authors: He, Xiaoxiao; Tang, Jiliang; et al. (2020)\n",
    "\n",
    "Purpose:\n",
    "End-to-end entry-point script for reproducing joint RNN-based early at-risk prediction:\n",
    "    - Environment and RNG seeding\n",
    "    - Data loading, preprocessing, and tensor assembly\n",
    "    - Baseline RNN and Joint GRU model definitions\n",
    "    - Cyclic training with per-course splits, CSV logs, and weights\n",
    "    - Overall and week-by-week evaluation metrics\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0b8a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 — Environment & RNG\n",
    "from datetime import datetime\n",
    "from tensorflow.python.client import device_lib\n",
    "import numpy as np, pandas as pd, tensorflow as tf, random, os, h5py, gc\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from keras.callbacks import CSVLogger, ModelCheckpoint\n",
    "from keras.layers import Input, Dense, GRU, Concatenate, LeakyReLU\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "print(\"TensorFlow build :\", tf.__version__)\n",
    "print(\"CUDA enabled     :\", tf.test.is_built_with_cuda())\n",
    "print(\"GPU available    :\", tf.test.is_gpu_available())\n",
    "\n",
    "local_devices = device_lib.list_local_devices()\n",
    "gpu_devices = [x.name for x in local_devices if x.device_type == 'GPU']\n",
    "print(\"Visible GPUs     :\", gpu_devices)\n",
    "\n",
    "SEED = 2025\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.set_random_seed(SEED)\n",
    "\n",
    "print(\"Session started  :\", datetime.now())\n",
    "\n",
    "import tensorflow as tf\n",
    "print(\"TF version:\", tf.__version__)\n",
    "print(\"Built with CUDA:\", tf.test.is_built_with_cuda())\n",
    "print(\"GPU available:\", tf.test.is_gpu_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d109de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 — Utilities & Training Config\n",
    "import os, h5py, numpy as np\n",
    "\n",
    "def _bytes(x):\n",
    "    return np.string_(x) if not isinstance(x, bytes) else x\n",
    "\n",
    "def _fix_attrs(h5obj):\n",
    "    for key in list(h5obj.attrs):\n",
    "        val = h5obj.attrs[key]\n",
    "        if isinstance(val, str):\n",
    "            del h5obj.attrs[key]\n",
    "            h5obj.attrs.create(key, _bytes(val))\n",
    "        elif isinstance(val, np.ndarray) and val.dtype.kind in ('U', 'O'):\n",
    "            del h5obj.attrs[key]\n",
    "            h5obj.attrs.create(key, np.array([_bytes(v) for v in val], dtype='S'))\n",
    "    for child in h5obj.values():\n",
    "        if isinstance(child, h5py.Group):\n",
    "            _fix_attrs(child)\n",
    "\n",
    "def safe_load_weights(model, path):\n",
    "    if not (path and os.path.exists(path) and h5py.is_hdf5(path)):\n",
    "        return\n",
    "    with h5py.File(path, 'r+') as f:\n",
    "        _fix_attrs(f)\n",
    "    model.load_weights(path)\n",
    "\n",
    "TRAINING_CONFIG = {\n",
    "    \"batch_size\": 256,\n",
    "    \"epochs\": 250,\n",
    "    \"optimizer\": \"adam\",\n",
    "    \"learning_rate\": 2e-5,\n",
    "    \"early_week\": 5,\n",
    "    \"last_week\": 39,\n",
    "}\n",
    "\n",
    "print(\"== Training configuration ==\")\n",
    "print(TRAINING_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825f622e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3 — Data Loading & Preprocessing\n",
    "# Step 3.1 — Read CSVs\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "DATA_DIR = '.'\n",
    "WEEKS   = 40\n",
    "\n",
    "def load_csv(filename):\n",
    "    path = os.path.join(DATA_DIR, filename)\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"{path} not found – double-check the folder.\")\n",
    "    return pd.read_csv(path)\n",
    "\n",
    "raw = {\n",
    "    \"assessments\"       : load_csv(\"assessments.csv\"),\n",
    "    \"courses\"           : load_csv(\"courses.csv\"),\n",
    "    \"studentAssessment\" : load_csv(\"studentAssessment.csv\"),\n",
    "    \"studentInfo\"       : load_csv(\"studentInfo.csv\"),\n",
    "    \"studentRegistration\": load_csv(\"studentRegistration.csv\"),\n",
    "    \"studentVle\"        : load_csv(\"studentVle.csv\"),\n",
    "    \"vle\"               : load_csv(\"vle.csv\"),\n",
    "}\n",
    "for k, df in raw.items():\n",
    "    print(f\"{k:<20} → {df.shape}\")\n",
    "\n",
    "# Step 3.2 — Target & Demographics\n",
    "stu_info = raw[\"studentInfo\"].copy()\n",
    "stu_info = stu_info[stu_info[\"final_result\"] != \"Withdrawn\"]\n",
    "PASS_LABELS = {\"Pass\", \"Distinction\"}\n",
    "stu_info[\"label\"] = np.where(stu_info[\"final_result\"].isin(PASS_LABELS), 1, 0)\n",
    "print(stu_info[\"label\"].value_counts(dropna=False))\n",
    "\n",
    "DEMOG_COLS = [\"gender\", \"region\", \"highest_education\", \"imd_band\", \"age_band\", \"disability\"]\n",
    "demog_onehot = pd.get_dummies(stu_info[DEMOG_COLS], prefix=DEMOG_COLS)\n",
    "print(f\"Demographic matrix shape  {demog_onehot.shape}\")\n",
    "\n",
    "# Step 3.3 — Weekly Assessment Matrix\n",
    "sa = (\n",
    "    raw[\"studentAssessment\"]\n",
    "    .merge(\n",
    "        raw[\"assessments\"][[\"id_assessment\", \"code_module\", \"code_presentation\"]],\n",
    "        on=\"id_assessment\", how=\"left\"\n",
    "    )\n",
    ")\n",
    "sa[\"week\"] = (sa[\"date_submitted\"] // 7).clip(lower=0, upper=WEEKS-1)\n",
    "ass_stream = (\n",
    "    sa.groupby([\"id_student\", \"code_module\", \"code_presentation\", \"week\"])[\"score\"]\n",
    "      .sum()\n",
    "      .reset_index()\n",
    "      .rename(columns={\"score\": \"score_sum\"})\n",
    ")\n",
    "ass_matrix = (\n",
    "    ass_stream\n",
    "    .pivot_table(index=[\"id_student\", \"code_module\", \"code_presentation\"],\n",
    "                 columns=\"week\", values=\"score_sum\", fill_value=0)\n",
    "    .reindex(columns=range(WEEKS), fill_value=0)\n",
    ")\n",
    "print(\"Assessment matrix shape:\", ass_matrix.shape)\n",
    "\n",
    "# Step 3.4 — Weekly Click Matrix\n",
    "sv = raw[\"studentVle\"].copy()\n",
    "sv[\"week\"] = (sv[\"date\"] // 7).clip(lower=0, upper=WEEKS-1)\n",
    "click_stream = (\n",
    "    sv.groupby([\"id_student\", \"code_module\", \"code_presentation\", \"week\"])[\"sum_click\"]\n",
    "      .sum()\n",
    "      .reset_index()\n",
    "      .rename(columns={\"sum_click\": \"clicks\"})\n",
    ")\n",
    "click_matrix = (\n",
    "    click_stream\n",
    "    .pivot_table(index=[\"id_student\", \"code_module\", \"code_presentation\"],\n",
    "                 columns=\"week\", values=\"clicks\", fill_value=0)\n",
    "    .reindex(columns=range(WEEKS), fill_value=0)\n",
    ")\n",
    "print(\"Click matrix shape:\", click_matrix.shape)\n",
    "\n",
    "# Step 3.5 — Final Design Matrix\n",
    "if not str(ass_matrix.columns[0]).startswith(\"ass_\"):\n",
    "    ass_matrix.columns = [f\"ass_{wk}\" for wk in ass_matrix.columns]\n",
    "if not str(click_matrix.columns[0]).startswith(\"click_\"):\n",
    "    click_matrix.columns = [f\"click_{wk}\" for wk in click_matrix.columns]\n",
    "ASS_COLS   = list(ass_matrix.columns)\n",
    "CLICK_COLS = list(click_matrix.columns)\n",
    "base_df = stu_info.set_index([\"id_student\", \"code_module\", \"code_presentation\"])\n",
    "demog_onehot.index = base_df.index\n",
    "merged = (\n",
    "    base_df\n",
    "    .join(demog_onehot, how=\"left\")\n",
    "    .join(ass_matrix,    how=\"left\")\n",
    "    .join(click_matrix,  how=\"left\")\n",
    ")\n",
    "merged[ASS_COLS + CLICK_COLS] = merged[ASS_COLS + CLICK_COLS].fillna(0)\n",
    "print(\"Final design-matrix shape:\", merged.shape)\n",
    "\n",
    "TARGET = merged[\"label\"].values\n",
    "DEMOG  = merged[demog_onehot.columns].values\n",
    "ASSESS = merged[ASS_COLS].values.reshape(-1, WEEKS, 1)\n",
    "CLICKS = merged[CLICK_COLS].values.reshape(-1, WEEKS, 1)\n",
    "\n",
    "print(\"DEMOG :\", DEMOG.shape,\n",
    "      \"\\nASSESS:\", ASSESS.shape,\n",
    "      \"\\nCLICKS:\", CLICKS.shape,\n",
    "      \"\\nLABEL :\", TARGET.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b64047c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4 — Model & Training\n",
    "# Step 4.1 — Shapes & Split Helper\n",
    "import numpy as np, pandas as pd, tensorflow as tf, random, os\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (Input, Dense, SimpleRNN, GRU, LSTM, Concatenate, LeakyReLU, Dropout)\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "SEED = 2025\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.set_random_seed(SEED)\n",
    "\n",
    "N_STUDENTS = TARGET.shape[0]\n",
    "DEMOG_DIM  = DEMOG.shape[1]\n",
    "TIME_STEPS = ASSESS.shape[1]\n",
    "SEQ_FEATS  = 1\n",
    "\n",
    "print(\"N_STUDENTS :\", N_STUDENTS, \"\\nDEMOG_DIM :\", DEMOG_DIM, \"\\nTIME_STEPS:\", TIME_STEPS)\n",
    "\n",
    "def make_course_split(current_mod, current_pres, val_ratio=0.20):\n",
    "    mask_prev = (\n",
    "        (merged.index.get_level_values(\"code_module\") <  current_mod) |\n",
    "        ((merged.index.get_level_values(\"code_module\") == current_mod) &\n",
    "         (merged.index.get_level_values(\"code_presentation\") < current_pres))\n",
    "    )\n",
    "    prev_idx = np.where(mask_prev)[0]\n",
    "    if prev_idx.size == 0:\n",
    "        raise ValueError(\"No historical courses before \"\n",
    "                         f\"{current_mod} {current_pres} to train on.\")\n",
    "    rng = np.random.RandomState(SEED)\n",
    "    rng.shuffle(prev_idx)\n",
    "    split = int((1 - val_ratio) * prev_idx.size)\n",
    "    train_idx, val_idx = prev_idx[:split], prev_idx[split:]\n",
    "    mask_test = (\n",
    "        (merged.index.get_level_values(\"code_module\") == current_mod) &\n",
    "        (merged.index.get_level_values(\"code_presentation\") == current_pres)\n",
    "    )\n",
    "    test_idx = np.where(mask_test)[0]\n",
    "    return train_idx, val_idx, test_idx\n",
    "\n",
    "# Step 4.2 — Baseline Builders\n",
    "def build_baseline(rnn_type=\"GRU\",\n",
    "                   demog_dim=DEMOG_DIM,\n",
    "                   time_steps=TIME_STEPS,\n",
    "                   seq_feats=SEQ_FEATS,\n",
    "                   lr=TRAINING_CONFIG[\"learning_rate\"]):\n",
    "    inp_demog  = Input(shape=(demog_dim,),  name=\"demographics\")\n",
    "    inp_assess = Input(shape=(time_steps, seq_feats), name=\"ass_seq\")\n",
    "    inp_click  = Input(shape=(time_steps, seq_feats), name=\"click_seq\")\n",
    "\n",
    "    x_dem = Dense(128)(inp_demog); x_dem = LeakyReLU()(x_dem)\n",
    "    x_dem = Dense(128)(x_dem);     x_dem = LeakyReLU()(x_dem)\n",
    "\n",
    "    RNNLayer = {\"SimpleRNN\": SimpleRNN, \"GRU\": GRU, \"LSTM\": LSTM}[rnn_type]\n",
    "\n",
    "    x_ass = inp_assess\n",
    "    for i in range(7):\n",
    "        return_seq = i < 6\n",
    "        x_ass = RNNLayer(256, return_sequences=return_seq, name=f\"ass_{rnn_type}_{i+1}\")(x_ass)\n",
    "\n",
    "    x_clk = inp_click\n",
    "    for i in range(7):\n",
    "        return_seq = i < 6\n",
    "        x_clk = RNNLayer(256, return_sequences=return_seq, name=f\"clk_{rnn_type}_{i+1}\")(x_clk)\n",
    "\n",
    "    concat = Concatenate()([x_dem, x_ass, x_clk])\n",
    "    y = Dense(384)(concat); y = LeakyReLU()(y)\n",
    "    y = Dense(768)(y);      y = LeakyReLU()(y)\n",
    "    y = Dense(1536)(y);     y = LeakyReLU()(y)\n",
    "    out = Dense(1, activation=\"sigmoid\")(y)\n",
    "\n",
    "    model = Model([inp_demog, inp_assess, inp_click], out, name=f\"Baseline_{rnn_type}\")\n",
    "    opt = Adam(lr=lr)\n",
    "    model.compile(optimizer=opt, loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "gru_baseline = build_baseline(\"GRU\")\n",
    "gru_baseline.summary(line_length=120)\n",
    "\n",
    "def build_joint_rnn_gru(demog_dim=DEMOG_DIM, time_steps=TIME_STEPS, seq_feats=SEQ_FEATS, lr=TRAINING_CONFIG[\"learning_rate\"]):\n",
    "    from keras.layers import Input, Dense, GRU, Concatenate, LeakyReLU\n",
    "    from keras.models import Model\n",
    "    from keras.optimizers import Adam\n",
    "    inp_demog  = Input(shape=(demog_dim,),            name=\"demographics\")\n",
    "    inp_assess = Input(shape=(time_steps, seq_feats), name=\"ass_seq\")\n",
    "    inp_click  = Input(shape=(time_steps, seq_feats), name=\"click_seq\")\n",
    "    x_dem = Dense(128)(inp_demog); x_dem = LeakyReLU()(x_dem)\n",
    "    x_dem = Dense(128)(x_dem);     x_dem = LeakyReLU()(x_dem)\n",
    "    shared = [GRU(256, return_sequences=(i < 6), name=f\"shared_GRU_{i+1}\") for i in range(7)]\n",
    "    def run_stack(x):\n",
    "        for layer in shared:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    x_ass = run_stack(inp_assess)\n",
    "    x_clk = run_stack(inp_click)\n",
    "    concat = Concatenate()([x_dem, x_ass, x_clk])\n",
    "    y = Dense(384)(concat);  y = LeakyReLU()(y)\n",
    "    y = Dense(768)(y);       y = LeakyReLU()(y)\n",
    "    y = Dense(1536)(y);      y = LeakyReLU()(y)\n",
    "    out = Dense(1, activation=\"sigmoid\")(y)\n",
    "    model = Model([inp_demog, inp_assess, inp_click], out, name=\"Joint_RNN_GRU\")\n",
    "    model.compile(optimizer=Adam(lr=lr), loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "# Step 4.3 — Cyclic Training, CSV Logs, Weights\n",
    "from keras.callbacks import CSVLogger, ModelCheckpoint\n",
    "import h5py, os, gc\n",
    "from tqdm import tqdm\n",
    "\n",
    "EPOCHS_TOTAL = TRAINING_CONFIG[\"epochs\"]\n",
    "CYCLE        = 50\n",
    "BATCH_SAFE   = 128\n",
    "\n",
    "SCHEDULE = [\n",
    "    (\"AAA\", \"2014J\"),\n",
    "    (\"BBB\", \"2014B\"), (\"BBB\", \"2014J\"),\n",
    "    (\"CCC\", \"2014J\"),\n",
    "    (\"DDD\", \"2014B\"), (\"DDD\", \"2014J\"),\n",
    "    (\"EEE\", \"2014B\"), (\"EEE\", \"2014J\"),\n",
    "    (\"FFF\", \"2014B\"), (\"FFF\", \"2014J\"),\n",
    "    (\"GGG\", \"2014B\"), (\"GGG\", \"2014J\"),\n",
    "]\n",
    "results = []\n",
    "\n",
    "def last_completed_epoch(csv_path):\n",
    "    if not os.path.exists(csv_path):\n",
    "        return -1\n",
    "    import pandas as pd\n",
    "    try:\n",
    "        return pd.read_csv(csv_path)[\"epoch\"].max()\n",
    "    except Exception:\n",
    "        return -1\n",
    "\n",
    "for mod, pres in tqdm(SCHEDULE, desc=\"Test courses\", unit=\"course\"):\n",
    "    tr_idx, val_idx, te_idx = make_course_split(mod, pres)\n",
    "    best_path = f\"{mod}_{pres}_best.weights.h5\"\n",
    "    csv_path  = f\"{mod}_{pres}.csv\"\n",
    "    ckpt_save  = ModelCheckpoint(best_path, monitor=\"val_loss\", save_best_only=True, save_weights_only=True, verbose=1)\n",
    "    csv_logger = CSVLogger(csv_path, append=True)\n",
    "    last_epoch = last_completed_epoch(csv_path)\n",
    "    START_FROM = max(0, (last_epoch + 1))\n",
    "    if START_FROM % CYCLE:\n",
    "        START_FROM = (START_FROM // CYCLE) * CYCLE\n",
    "    print(f\"\\n{mod} {pres}: resuming at epoch {START_FROM}\")\n",
    "    for start in range(START_FROM, EPOCHS_TOTAL, CYCLE):\n",
    "        span  = min(CYCLE, EPOCHS_TOTAL - start)\n",
    "        model = build_joint_rnn_gru()\n",
    "        safe_load_weights(model, best_path)\n",
    "        print(f\"Training from epoch {start} to {start + span - 1} for {mod} {pres}\")\n",
    "        model.fit([DEMOG[tr_idx],  ASSESS[tr_idx],  CLICKS[tr_idx]],\n",
    "                  TARGET[tr_idx],\n",
    "                  validation_data=([DEMOG[val_idx], ASSESS[val_idx], CLICKS[val_idx]], TARGET[val_idx]),\n",
    "                  batch_size=BATCH_SAFE,\n",
    "                  epochs=start + span,\n",
    "                  initial_epoch=start,\n",
    "                  verbose=0,\n",
    "                  callbacks=[csv_logger, ckpt_save])\n",
    "        print(f\"Finished training span for {mod} {pres}\")\n",
    "        K.clear_session(); gc.collect()\n",
    "    model = build_joint_rnn_gru()\n",
    "    safe_load_weights(model, best_path)\n",
    "    y_pred = (model.predict([DEMOG[te_idx], ASSESS[te_idx], CLICKS[te_idx]], batch_size=BATCH_SAFE) > 0.5).ravel()\n",
    "    acc  = accuracy_score(TARGET[te_idx], y_pred)\n",
    "    prec = precision_score(TARGET[te_idx], y_pred, zero_division=0)\n",
    "    rec  = recall_score  (TARGET[te_idx], y_pred, zero_division=0)\n",
    "    results.append((mod, pres, acc, prec, rec))\n",
    "    print(f\"{mod} {pres} → acc {acc:.3f} | prec {prec:.3f} | rec {rec:.3f}\")\n",
    "\n",
    "acc_m, prec_m, rec_m = np.mean(np.array([[a,p,r] for _,_,a,p,r in results]),0)\n",
    "print(\"\\n=== Overall ===\")\n",
    "print(f\"Accuracy  {acc_m:.3f}\")\n",
    "print(f\"Precision {prec_m:.3f}\")\n",
    "print(f\"Recall    {rec_m:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82ac3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5 — Week-by-week Evaluation\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "W_START = TRAINING_CONFIG[\"early_week\"]\n",
    "W_END   = TRAINING_CONFIG[\"last_week\"]\n",
    "curves   = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "for w in range(W_START, W_END + 1):\n",
    "    assess_masked = ASSESS[te_idx].copy()\n",
    "    clicks_masked = CLICKS[te_idx].copy()\n",
    "    if w < W_END:\n",
    "        assess_masked[:, w+1:, :] = 0\n",
    "        clicks_masked[:, w+1:, :] = 0\n",
    "    y_pred = (model.predict([DEMOG[te_idx], assess_masked, clicks_masked], batch_size=BATCH_SAFE, verbose=0) > 0.5).ravel()\n",
    "    curves[w][\"acc\"].append (accuracy_score (TARGET[te_idx], y_pred))\n",
    "    curves[w][\"prec\"].append(precision_score(TARGET[te_idx], y_pred, zero_division=0))\n",
    "    curves[w][\"rec\"].append (recall_score   (TARGET[te_idx], y_pred, zero_division=0))\n",
    "\n",
    "weeks   = np.arange(W_START, W_END + 1)\n",
    "acc_avg = [np.mean(curves[w][\"acc\"])  for w in weeks]\n",
    "pre_avg = [np.mean(curves[w][\"prec\"]) for w in weeks]\n",
    "rec_avg = [np.mean(curves[w][\"rec\"])  for w in weeks]\n",
    "\n",
    "import pandas as pd\n",
    "metrics_df = pd.DataFrame({\"week\": weeks, \"accuracy\": acc_avg, \"precision\": pre_avg, \"recall\": rec_avg})\n",
    "print(\"\\nWeek-by-week metrics (averaged across courses)\")\n",
    "print(metrics_df.head())\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(weeks, acc_avg,  label=\"Accuracy\")\n",
    "plt.plot(weeks, pre_avg,  label=\"Precision\")\n",
    "plt.plot(weeks, rec_avg,  label=\"Recall\")\n",
    "plt.xlabel(\"Week of course (0-39)\")\n",
    "plt.ylabel(\"Metric\")\n",
    "plt.title(\"Online at-risk prediction – Joint GRU (reproduction)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
