{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a208c4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Paper 1 Reproduction Script\n",
    "\"\"\"\n",
    "Title: \"A Machine Learning Based Approach for Student Performance Evaluation in Educational Data Mining\"\n",
    "Authors: Muhammad Sammy Ahmad, Ahmed H. Asad, Ammar Mohammed\n",
    "Source: Proceedings of the 2021 International Mobile, Intelligent, and Ubiquitous Computing Conference (MIUCC)\n",
    "DOI: 10.1109/MIUCC52538.2021.9447602\n",
    "\n",
    "Purpose:\n",
    "This script reproduces the methodology and results of the above paper using the \n",
    "Open University Learning Analytics Dataset (OULAD). It executes the complete \n",
    "pipeline described in the paper:\n",
    "    - Data loading and preprocessing\n",
    "    - Feature engineering\n",
    "    - Model training (Artificial Neural Network, Random Forest)\n",
    "    - Evaluation (accuracy, classification metrics, cross-validation)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9221f4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5711e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "try:\n",
    "    tf.random.set_seed(42)      # TF 2.x\n",
    "except AttributeError:\n",
    "    tf.set_random_seed(42)      # TF 1.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795c66de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Data Manipulation (using the exact v1.0 OULAD URLs)\n",
    "\n",
    "# 1. Load the exact v1.0 CSVs from the GitHub mirror\n",
    "url_info   = \"https://raw.githubusercontent.com/marloft/MachineLearning/master/Documents/ML/PhD/Datasets/Open%20University%20Learning%20Analytics%20Dataset%20-%20OULAD/studentInfo.csv\"\n",
    "url_assess = \"https://raw.githubusercontent.com/marloft/MachineLearning/master/Documents/ML/PhD/Datasets/Open%20University%20Learning%20Analytics%20Dataset%20-%20OULAD/studentAssessment.csv\"\n",
    "\n",
    "df_info   = pd.read_csv(url_info)\n",
    "df_assess = pd.read_csv(url_assess)\n",
    "\n",
    "# 2. Keep only Distinction/Pass/Fail and drop Ireland & North Region (as per paper)\n",
    "df_info = df_info[df_info[\"final_result\"].isin([\"Distinction\", \"Pass\", \"Fail\"])]\n",
    "df_info = df_info[~df_info[\"region\"].isin([\"Ireland\", \"North Region\"])]\n",
    "\n",
    "# 3. Aggregate assessment features per student:\n",
    "agg_dict = {\n",
    "    \"date_submitted\": \"mean\",\n",
    "    \"is_banked\":       \"sum\",\n",
    "    \"id_assessment\":   \"count\",\n",
    "    \"score\":           \"mean\"\n",
    "}\n",
    "if \"num_of_prev_attempts\" in df_assess.columns:\n",
    "    agg_dict[\"num_of_prev_attempts\"] = \"sum\"\n",
    "\n",
    "df_assess_agg = (\n",
    "    df_assess\n",
    "    .groupby(\"id_student\")\n",
    "    .agg(agg_dict)\n",
    "    .rename(columns={\n",
    "        \"id_assessment\": \"num_assessments\",\n",
    "        \"score\":          \"avg_score\"\n",
    "    })\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# 4. Merge back to df_info on id_student\n",
    "df_merged = pd.merge(\n",
    "    df_info,\n",
    "    df_assess_agg,\n",
    "    how=\"inner\",\n",
    "    on=\"id_student\"\n",
    ")\n",
    "\n",
    "# 5. Drop any rows with NaNs\n",
    "df_merged = df_merged.dropna()\n",
    "\n",
    "# 6. Save the cleaned v1.0 merge for Step 3\n",
    "df_merged.to_csv(\"Step2_CleanedDS_v1.csv\", index=False)\n",
    "print(\"Step 2 (v1.0) complete. Shape:\", df_merged.shape)\n",
    "print(\"Saved as Step2_CleanedDS_v1.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49fe170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Feature Engineering (using Step2_CleanedDS_v1.csv)\n",
    "\n",
    "# 1. Load the v1.0 cleaned, merged dataset from Step 2\n",
    "df = pd.read_csv(\"Step2_CleanedDS_v1.csv\")\n",
    "\n",
    "# 2. Drop \"id_student\"\n",
    "df = df.drop(columns=[\"id_student\"], errors=\"ignore\")\n",
    "\n",
    "# 3. Encode the target variable \"final_result\": Distinction→0, Fail→1, Pass→2\n",
    "df[\"final_result\"] = df[\"final_result\"].map({\n",
    "    \"Distinction\": 0,\n",
    "    \"Fail\":        1,\n",
    "    \"Pass\":        2\n",
    "})\n",
    "\n",
    "# 4. Label-encode binary features: gender (F→0, M→1), disability (N→0, Y→1)\n",
    "df[\"gender\"]     = df[\"gender\"].map({\"F\": 0, \"M\": 1})\n",
    "df[\"disability\"] = df[\"disability\"].map({\"N\": 0, \"Y\": 1})\n",
    "\n",
    "# 5a. Ordinal-encode highest_education\n",
    "edu_map = {\n",
    "    \"No Formal quals\":        0,\n",
    "    \"Lower Than A Level\":     1,\n",
    "    \"A Level or Equivalent\":  2,\n",
    "    \"HE Qualification\":       3,\n",
    "    \"Post Graduate Qualification\": 4\n",
    "}\n",
    "df[\"highest_education\"] = df[\"highest_education\"].map(edu_map)\n",
    "\n",
    "# 5b. Ordinal-encode age_band\n",
    "age_map = {\"0-35\": 0, \"35-55\": 1, \"55<=\": 2}\n",
    "df[\"age_band\"] = df[\"age_band\"].map(age_map)\n",
    "\n",
    "# 5c. Ordinal-encode imd_band\n",
    "imd_map = {\n",
    "    \"0-10%\":   0,\n",
    "    \"10-20%\":  1,\n",
    "    \"20-30%\":  2,\n",
    "    \"30-40%\":  3,\n",
    "    \"40-50%\":  4,\n",
    "    \"50-60%\":  5,\n",
    "    \"60-70%\":  6,\n",
    "    \"70-80%\":  7,\n",
    "    \"80-90%\":  8,\n",
    "    \"90-100%\": 9\n",
    "}\n",
    "df[\"imd_band\"] = df[\"imd_band\"].map(imd_map)\n",
    "\n",
    "# 6. Ensure \"date_submitted\" is numeric (v1.0 files already have it numeric)\n",
    "if df[\"date_submitted\"].dtype == object:\n",
    "    df[\"date_submitted\"] = pd.to_datetime(df[\"date_submitted\"], errors=\"coerce\")\n",
    "    df[\"date_submitted\"] = df[\"date_submitted\"].astype(np.int64) // 10**9\n",
    "\n",
    "# 7. One-hot encode region, code_module, code_presentation (drop_first=True)\n",
    "df_featured = pd.get_dummies(\n",
    "    df,\n",
    "    columns=[\"region\", \"code_module\", \"code_presentation\"],\n",
    "    drop_first=True\n",
    ")\n",
    "\n",
    "# 8. Fill any remaining NaNs with 0\n",
    "df_featured = df_featured.fillna(0)\n",
    "\n",
    "# 9. Verify number of input features (excluding \"final_result\")\n",
    "features = [c for c in df_featured.columns if c != \"final_result\"]\n",
    "print(\"Number of feature columns (should be 31):\", len(features))\n",
    "\n",
    "# 10. Save the feature-engineered dataset for Step 4\n",
    "df_featured.to_csv(\"Step3_FeatureEngineered_v1.csv\", index=False)\n",
    "print(\"Step 3 (v1.0) complete. Saved as Step3_FeatureEngineered_v1.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8d252c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 4: ANN\n",
    "\n",
    "# 1. Load the feature‐engineered dataset from Step 3\n",
    "df = pd.read_csv(\"Step3_FeatureEngineered_v1.csv\")\n",
    "\n",
    "# 2. Separate X (features) and y (target)\n",
    "#    – Drop “final_result” from X\n",
    "X = df.drop(columns=[\"final_result\"]).values   # 30 columns\n",
    "y = df[\"final_result\"].values                  # values in {0,1,2}\n",
    "\n",
    "# 3. Normalize features (StandardScaler)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# 4. Split into Train/Test (80/20), stratified by y\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y,\n",
    "    test_size=0.20,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "# 5. One‐hot encode the targets for Keras\n",
    "y_train_cat = to_categorical(y_train, num_classes=3)\n",
    "y_test_cat  = to_categorical(y_test,  num_classes=3)\n",
    "\n",
    "# 6. Build the ANN model function\n",
    "#    – 30 inputs → four hidden layers of 128 units, ReLU → output layer of 3 with softmax\n",
    "def build_ann_model(input_dim):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, input_dim=input_dim, activation=\"relu\"))\n",
    "    model.add(Dense(128, activation=\"relu\"))\n",
    "    model.add(Dense(128, activation=\"relu\"))\n",
    "    model.add(Dense(128, activation=\"relu\"))\n",
    "    model.add(Dense(3,   activation=\"softmax\"))\n",
    "    model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# 7. Instantiate and train the ANN (using 10% of training as validation)\n",
    "input_dim = X_train.shape[1]  # should be 30\n",
    "model = build_ann_model(input_dim=input_dim)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train_cat,\n",
    "    validation_split=0.10,    # hold out 10% of training for validation\n",
    "    epochs=30,                # same number of epochs as paper\n",
    "    batch_size=64,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 8. Evaluate on the held‐out test set\n",
    "y_pred_probs = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "print(\"\\n=== ANN Test‐Set Classification Report ===\")\n",
    "print(classification_report(y_test, y_pred_classes, target_names=[\"Distinction\",\"Fail\",\"Pass\"]))\n",
    "\n",
    "print(\"\\n=== ANN Test‐Set Confusion Matrix ===\")\n",
    "print(confusion_matrix(y_test, y_pred_classes))\n",
    "\n",
    "# 9. 10‐Fold Stratified Cross‐Validation (ANN) with 30 features\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "cv_accuracies = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X_scaled, y), start=1):\n",
    "    print(f\"\\nTraining fold {fold}/10...\")\n",
    "    X_tr, X_val = X_scaled[train_idx], X_scaled[val_idx]\n",
    "    y_tr, y_val = y[train_idx], y[val_idx]\n",
    "    \n",
    "    y_tr_cat = to_categorical(y_tr, num_classes=3)\n",
    "    \n",
    "    model_cv = build_ann_model(input_dim=input_dim)\n",
    "    model_cv.fit(\n",
    "        X_tr, y_tr_cat,\n",
    "        epochs=30,\n",
    "        batch_size=64,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    val_pred_probs = model_cv.predict(X_val)\n",
    "    val_pred_classes = np.argmax(val_pred_probs, axis=1)\n",
    "    fold_acc = accuracy_score(y_val, val_pred_classes)\n",
    "    cv_accuracies.append(fold_acc)\n",
    "    print(f\"Fold {fold} accuracy: {fold_acc:.4f}\")\n",
    "\n",
    "print(\"\\n=== ANN 10‐Fold CV Results ===\")\n",
    "print(f\"Average CV accuracy: {np.mean(cv_accuracies):.4f}\")\n",
    "print(f\"CV accuracy variance: {np.var(cv_accuracies):.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611a7629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Random Forest Classification with 30 Input Features\n",
    "\n",
    "# 1. Load the feature‐engineered dataset from Step 3\n",
    "df = pd.read_csv(\"Step3_FeatureEngineered_v1.csv\")\n",
    "\n",
    "# 2. Separate X (features) and y (target)\n",
    "X = df.drop(columns=[\"final_result\"]).values   # 30 input columns\n",
    "y = df[\"final_result\"].values                  # {0, 1, 2}\n",
    "\n",
    "# 3. (Optional) Normalize features\n",
    "#    Random Forests do not require scaling, but we include it for consistency\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# 4. Split into Train/Test (80/20), stratified by y\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y,\n",
    "    test_size=0.20,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "# 5. Train the Random Forest\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=500,\n",
    "    criterion='entropy',\n",
    "    max_depth=None,\n",
    "    random_state=42\n",
    ")\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# 6. Evaluate on the held‐out test set\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "print(\"\\n=== RF Test‐Set Classification Report ===\")\n",
    "print(classification_report(y_test, y_pred, target_names=[\"Distinction\",\"Fail\",\"Pass\"]))\n",
    "\n",
    "print(\"\\n=== RF Test‐Set Confusion Matrix ===\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# 7. 10‐Fold Stratified Cross‐Validation (RF)\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "rf_accuracies = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X_scaled, y), start=1):\n",
    "    print(f\"\\nTraining RF fold {fold}/10...\")\n",
    "    X_tr, X_val = X_scaled[train_idx], X_scaled[val_idx]\n",
    "    y_tr, y_val = y[train_idx], y[val_idx]\n",
    "    \n",
    "    rf_cv = RandomForestClassifier(\n",
    "        n_estimators=500,\n",
    "        criterion='entropy',\n",
    "        max_depth=None,\n",
    "        random_state=42\n",
    "    )\n",
    "    rf_cv.fit(X_tr, y_tr)\n",
    "    \n",
    "    val_pred = rf_cv.predict(X_val)\n",
    "    fold_acc = accuracy_score(y_val, val_pred)\n",
    "    rf_accuracies.append(fold_acc)\n",
    "    print(f\"Fold {fold} accuracy: {fold_acc:.4f}\")\n",
    "\n",
    "print(\"\\n=== RF 10‐Fold CV Results ===\")\n",
    "print(f\"Average CV accuracy: {np.mean(rf_accuracies):.4f}\")\n",
    "print(f\"CV accuracy variance: {np.var(rf_accuracies):.6f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
