{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c82f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paper 6 Reproduction Script\n",
    "\"\"\"\n",
    "Title: \"Ouroboros: early identification of at-risk students without models based on legacy data\"\n",
    "Authors: Martin Hlosta, Zdenek Zdrahal, Jaroslav Zendulka (LAK 2017)\n",
    "\n",
    "Environment (pinned):\n",
    "    Python 3.7.x\n",
    "    scikit-learn==0.24.2\n",
    "    xgboost==1.3.3\n",
    "    pandas==1.2.3\n",
    "    numpy==1.19.5\n",
    "    imbalanced-learn==0.8.0\n",
    "    matplotlib==3.3.4\n",
    "    seaborn==0.11.1\n",
    "    joblib\n",
    "\n",
    "Purpose:\n",
    "End-to-end entry-point script for A1 submission prediction with self-learning (Ouroboros):\n",
    "    - Data loading and filtering\n",
    "    - Feature engineering\n",
    "    - Windowed training/evaluation (PR-AUC)\n",
    "    - Top-K precision/recall\n",
    "    - XGBoost feature importances\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2cfffc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Tuple\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import clone\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import average_precision_score, precision_score, recall_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm      import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09760a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNG seeds\n",
    "random.seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e61c185",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_info       = pd.read_csv(\"studentInfo.csv\")\n",
    "student_reg        = pd.read_csv(\"studentRegistration.csv\")\n",
    "student_reg['date_unregistration'].replace(0, np.nan, inplace=True)\n",
    "student_assessment = pd.read_csv(\"studentAssessment.csv\")\n",
    "assessments        = pd.read_csv(\"assessments.csv\")\n",
    "student_vle        = pd.read_csv(\"studentVle.csv\")\n",
    "vle                = pd.read_csv(\"vle.csv\")\n",
    "courses            = pd.read_csv(\"courses.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81777af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Data Preparation & Filtering\n",
    "\n",
    "# 3.1 Filter to 2014J and modules B,D,E,F\n",
    "presentation = '2014J'\n",
    "modules = ['BBB','DDD','EEE','FFF']\n",
    "si = student_info[\n",
    "    (student_info.code_module.isin(modules)) &\n",
    "    (student_info.code_presentation == presentation)\n",
    "].copy()\n",
    "\n",
    "# 3.2 Identify first assessment (A1) per module\n",
    "asmt = assessments[\n",
    "    (assessments.code_module.isin(modules)) &\n",
    "    (assessments.code_presentation == presentation) &\n",
    "    (assessments.assessment_type == 'TMA') &\n",
    "    (assessments.weight > 0)\n",
    "].copy()\n",
    "\n",
    "first_asmt = (\n",
    "    asmt.sort_values(['code_module', 'date'])\n",
    "        .groupby('code_module')\n",
    "        .first()[['id_assessment', 'date']]\n",
    "        .rename(columns={'id_assessment': 'A1_id', 'date': 'A1_cutoff'})\n",
    "        .reset_index()\n",
    ")\n",
    "\n",
    "A1_ids = set(first_asmt['A1_id'])\n",
    "\n",
    "sa = (\n",
    "    student_assessment\n",
    "      .merge(assessments[['id_assessment','code_module','code_presentation']],\n",
    "             on='id_assessment', how='left')\n",
    "      .query(\"code_module in @modules and code_presentation == @presentation\")\n",
    "      .query(\"id_assessment in @A1_ids\")\n",
    ")\n",
    "\n",
    "# 3.3 Build students frame with cutoff, registration, earliest submission\n",
    "students = (\n",
    "    si[['id_student','code_module']]\n",
    "      .drop_duplicates()\n",
    "      .merge(first_asmt, on='code_module', how='left')\n",
    "      .merge(\n",
    "          student_reg[['id_student','code_module',\n",
    "                       'date_registration',\n",
    "                       'date_unregistration']],\n",
    "          on=['id_student','code_module'], how='left')\n",
    ")\n",
    "\n",
    "subs = (\n",
    "    sa.groupby(['id_student','code_module'])['date_submitted']\n",
    "      .min()\n",
    "      .reset_index()\n",
    "      .rename(columns={'date_submitted':'submission_date'})\n",
    ")\n",
    "\n",
    "students = students.merge(subs, on=['id_student','code_module'], how='left')\n",
    "students['will_submit'] = (\n",
    "    (students['submission_date'] <= students['A1_cutoff'])\n",
    ").fillna(False).astype(int)\n",
    "\n",
    "train_sets = {}\n",
    "for d in range(12):\n",
    "    df = students.copy()\n",
    "    df['prediction_day'] = df['A1_cutoff'] - d\n",
    "    df['window'] = d\n",
    "    train_sets[d] = df[['id_student','code_module','prediction_day','A1_cutoff','will_submit','window']]\n",
    "\n",
    "print(first_asmt)\n",
    "print(students.head())\n",
    "print(train_sets[0].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c51a384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Feature Engineering\n",
    "\n",
    "demo_cols = [\n",
    "    'gender', 'region', 'highest_education', 'imd_band',\n",
    "    'age_band', 'num_of_prev_attempts', 'studied_credits', 'disability'\n",
    "]\n",
    "\n",
    "def add_vle_stats(df_clicks: pd.DataFrame, prefix: str) -> pd.DataFrame:\n",
    "    stats = (\n",
    "        df_clicks\n",
    "        .groupby(['id_student', 'code_module'])\n",
    "        .agg(\n",
    "            first_login   = ('date', 'min'),\n",
    "            last_login    = ('date', 'max'),\n",
    "            streak        = ('date', lambda x: (\n",
    "                               x.sort_values()\n",
    "                                .diff().eq(1)\n",
    "                                .groupby((x.diff() != 1).cumsum())\n",
    "                                .size()\n",
    "                                .max())),\n",
    "            total_days    = ('date', 'nunique'),\n",
    "            sum_clicks    = ('sum_click', 'sum'),\n",
    "            mean_clicks   = ('sum_click', 'mean'),\n",
    "            median_clicks = ('sum_click', 'median'),\n",
    "            max_clicks    = ('sum_click', 'max'),\n",
    "            min_clicks    = ('sum_click', 'min')\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "    metric_cols = [c for c in stats.columns if c not in ['id_student', 'code_module']]\n",
    "    return stats.rename(columns={c: f'{prefix}{c}' for c in metric_cols})\n",
    "\n",
    "def make_pipe(base_clf, X_sample):\n",
    "    num_cols = X_sample.select_dtypes(include=['number']).columns\n",
    "    cat_cols = X_sample.select_dtypes(exclude=['number']).columns\n",
    "    cat_pipeline = Pipeline([\n",
    "        ('impute', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "        ('cast',   FunctionTransformer(lambda x: x.astype(str), validate=False)),\n",
    "        ('ohe',    OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "    prep = ColumnTransformer([\n",
    "        ('num', StandardScaler(), num_cols),\n",
    "        ('cat', cat_pipeline,    cat_cols)\n",
    "    ])\n",
    "    return Pipeline([('prep', prep), ('clf', clone(base_clf))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847be0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Ouroboros Logic\n",
    "\n",
    "def make_window_core(students_df: pd.DataFrame, d: int) -> pd.DataFrame:\n",
    "    core = students_df.copy()\n",
    "    core['prediction_day'] = core['A1_cutoff'] - (d + 1)\n",
    "    core['window']         = d\n",
    "    return core\n",
    "\n",
    "def filter_not_yet_submitted(core: pd.DataFrame) -> pd.DataFrame:\n",
    "    keep = (\n",
    "        (core['date_registration'] <= core['prediction_day']) &\n",
    "        (core['submission_date'].isna() | (core['submission_date'] > core['prediction_day'])) &\n",
    "        (core['date_unregistration'].isna() | (core['date_unregistration'] >= core['prediction_day']))\n",
    "    )\n",
    "    return core.loc[keep].copy()\n",
    "\n",
    "def add_label(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df['will_submit_window'] = (\n",
    "        (df['submission_date'] > df['prediction_day']) &\n",
    "        (df['submission_date'] <= df['A1_cutoff'])\n",
    "    ).fillna(False).astype(int)\n",
    "    return df\n",
    "\n",
    "def build_features_for_window(core: pd.DataFrame) -> Tuple[pd.DataFrame, pd.Series]:\n",
    "    df_dem = core.drop_duplicates(['id_student','code_module']).merge(\n",
    "        si[['id_student','code_module'] + demo_cols],\n",
    "        on=['id_student','code_module'], how='left'\n",
    "    )\n",
    "    df_reg = df_dem.copy()\n",
    "    df_reg['days_since_start'] = (df_reg['prediction_day'] - df_reg['date_registration'])\n",
    "    sv = (\n",
    "        student_vle\n",
    "          .merge(vle[['id_site','activity_type']], on='id_site', how='left')\n",
    "          .merge(df_reg[['id_student','code_module','prediction_day']],\n",
    "                 on=['id_student','code_module'], how='right')\n",
    "    )\n",
    "    sv = sv[sv['date'] <= sv['prediction_day']]\n",
    "    vle_stats = add_vle_stats(sv[['id_student','code_module','date','sum_click']], prefix='vle_')\n",
    "    agg = (\n",
    "        sv.groupby(['id_student','code_module'])\n",
    "          .agg(total_clicks=('sum_click','sum'),\n",
    "               active_days =('sum_click', lambda x: (x > 0).sum()),\n",
    "               avg_clicks  =('sum_click','mean'))\n",
    "          .reset_index()\n",
    "    )\n",
    "    act = (\n",
    "        sv.groupby(['id_student','code_module','activity_type'])['sum_click']\n",
    "          .sum()\n",
    "          .unstack(fill_value=0)\n",
    "          .reset_index()\n",
    "    )\n",
    "    df_vle_agg = (\n",
    "        df_reg\n",
    "          .merge(agg,       on=['id_student','code_module'], how='left')\n",
    "          .merge(act,       on=['id_student','code_module'], how='left')\n",
    "          .merge(vle_stats, on=['id_student','code_module'], how='left')\n",
    "          .fillna(0)\n",
    "    )\n",
    "    sv_pre = student_vle[\n",
    "        (student_vle.code_module.isin(modules)) &\n",
    "        (student_vle.code_presentation == presentation) &\n",
    "        (student_vle.date < 0)\n",
    "    ]\n",
    "    pre_stats = add_vle_stats(sv_pre[['id_student','code_module','date','sum_click']], prefix='pre_')\n",
    "    pre_sum = (\n",
    "        sv_pre.groupby(['id_student','code_module'])['sum_click']\n",
    "              .sum()\n",
    "              .reset_index()\n",
    "              .rename(columns={'sum_click':'pre_presentation_clicks'})\n",
    "    )\n",
    "    df_pre = (\n",
    "        df_vle_agg\n",
    "          .merge(pre_sum,   on=['id_student','code_module'], how='left')\n",
    "          .merge(pre_stats, on=['id_student','code_module'], how='left')\n",
    "          .fillna(0)\n",
    "    )\n",
    "    N_days = 60\n",
    "    sv_ta = (\n",
    "        student_vle\n",
    "          .merge(vle[['id_site','activity_type']], on='id_site', how='left')\n",
    "          .merge(core[['id_student','code_module','prediction_day']],\n",
    "                 on=['id_student','code_module'], how='right')\n",
    "    )\n",
    "    sv_ta['rel_day'] = sv_ta['date'] - sv_ta['prediction_day']\n",
    "    sv_ta = sv_ta[(sv_ta['rel_day'] <= 0) & (sv_ta['rel_day'] >= -N_days)]\n",
    "    pivot = sv_ta.pivot_table(\n",
    "        index=['id_student','code_module'],\n",
    "        columns='rel_day',\n",
    "        values='sum_click',\n",
    "        aggfunc='sum',\n",
    "        fill_value=0\n",
    "    ).reset_index()\n",
    "    pivot.columns = ['id_student','code_module'] + [f'clicks_day{int(c)}' for c in pivot.columns[2:]]\n",
    "    df_final = (\n",
    "        df_pre.merge(pivot, on=['id_student','code_module'], how='left')\n",
    "              .fillna(0)\n",
    "              .drop_duplicates(['id_student','code_module'])\n",
    "    )\n",
    "    idx = df_final.set_index(['id_student','code_module']).index\n",
    "    y = (core\n",
    "         .set_index(['id_student','code_module'])['will_submit_window']\n",
    "         .groupby(level=[0,1]).first()\n",
    "         .reindex(idx))\n",
    "    X = df_final.drop(columns=[\n",
    "        'id_student','code_module','prediction_day','A1_cutoff',\n",
    "        'date_registration','date_unregistration',\n",
    "        'submission_date','will_submit','will_submit_window'\n",
    "    ], errors='ignore')\n",
    "    return X.reset_index(drop=True), y.reset_index(drop=True)\n",
    "\n",
    "core = make_window_core(students, 0)\n",
    "print(\"total before filter:\", len(core))\n",
    "print(\"< today:\",  (core['submission_date'] <  core['prediction_day']).sum())\n",
    "print(\"= today:\",  (core['submission_date'] == core['prediction_day']).sum())\n",
    "print(\"> today:\",  (core['submission_date'] >  core['prediction_day']).sum(), \"|  NaN:\",   core['submission_date'].isna().sum())\n",
    "\n",
    "core = filter_not_yet_submitted(core)\n",
    "print(\"after filter:\", len(core))\n",
    "core = add_label(core)\n",
    "print(\"positives:\", core['will_submit_window'].sum())\n",
    "\n",
    "# 5.1 Window sanity check\n",
    "for d in range(12):\n",
    "    core = make_window_core(students, d)\n",
    "    core = filter_not_yet_submitted(core)\n",
    "    core = add_label(core)\n",
    "    print(f\"d={d:2d} rows={len(core):4d}  positives={core['will_submit_window'].sum():4d}\")\n",
    "\n",
    "window_datasets = {}\n",
    "for d in range(12):\n",
    "    core = make_window_core(students, d)\n",
    "    core = filter_not_yet_submitted(core)\n",
    "    core = add_label(core)\n",
    "    X_d, y_d = build_features_for_window(core)\n",
    "    window_datasets[d] = (X_d, y_d)\n",
    "    print(f\"d = {d:2d}   rows = {len(y_d):4d}   positives = {y_d.sum():4d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c08e632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Model Training & PR-AUC\n",
    "\n",
    "models = {\n",
    "    \"LR\"  : LogisticRegression(max_iter=1000, class_weight=\"balanced\"),\n",
    "    \"SVM\" : SVC(kernel=\"rbf\", probability=True, class_weight=\"balanced\"),\n",
    "    \"RF\"  : RandomForestClassifier(n_estimators=200, n_jobs=-1, class_weight=\"balanced\"),\n",
    "    \"XGB\" : XGBClassifier(n_estimators=400, learning_rate=0.05, max_depth=6,\n",
    "                          subsample=0.8, colsample_bytree=0.8, n_jobs=-1, eval_metric=\"logloss\"),\n",
    "    \"NB\"  : GaussianNB(),\n",
    "}\n",
    "\n",
    "results = []\n",
    "for d, (X, y) in window_datasets.items():\n",
    "    if y.sum() == 0 or y.sum() == len(y):\n",
    "        print(f\"⚠︎  window {d} skipped (positives = {y.sum()})\")\n",
    "        continue\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.30, stratify=y, random_state=42)\n",
    "    for name, base_clf in models.items():\n",
    "        clf = clone(base_clf)\n",
    "        if name == \"XGB\":\n",
    "            pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "            clf.set_params(scale_pos_weight=pos_weight)\n",
    "        pipe = make_pipe(clf, X_train)\n",
    "        pipe.fit(X_train, y_train)\n",
    "        y_score = pipe.predict_proba(X_test)[:, 1]\n",
    "        pr_auc  = average_precision_score(y_test, y_score)\n",
    "        results.append({\n",
    "            \"window\"   : d,\n",
    "            \"model\"    : name,\n",
    "            \"rows\"     : len(y_test),\n",
    "            \"positives\": int(y_test.sum()),\n",
    "            \"PR_AUC\"   : round(pr_auc, 4),\n",
    "        })\n",
    "        print(f\"d={d:2d}  {name:3s}  PR-AUC = {pr_auc:6.4f}\")\n",
    "\n",
    "summary = (pd.DataFrame(results)\n",
    "           .pivot(index=\"window\", columns=\"model\", values=\"PR_AUC\")\n",
    "           .sort_index())\n",
    "print(\"\\n===  PR-AUC by window ===\")\n",
    "print(summary.to_string(float_format=\"%.4f\"))\n",
    "\n",
    "student_vle.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107d8d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Top-K Precision & Recall\n",
    "\n",
    "top_ks = [0.05, 0.10, 0.25]\n",
    "results_topk = []\n",
    "\n",
    "base_clf = XGBClassifier(\n",
    "    n_estimators=400,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=6,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    n_jobs=-1,\n",
    "    eval_metric=\"logloss\"\n",
    ")\n",
    "\n",
    "for d, (X, y) in window_datasets.items():\n",
    "    if y.sum() in [0, len(y)]:\n",
    "        continue\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "        X, y, test_size=0.3, stratify=y, random_state=42\n",
    "    )\n",
    "    w = (y_tr==0).sum() / (y_tr==1).sum()\n",
    "    clf = clone(base_clf)\n",
    "    clf.set_params(scale_pos_weight=w)\n",
    "    pipe = make_pipe(clf, X_tr)\n",
    "    pipe.fit(X_tr, y_tr)\n",
    "    scores = pipe.predict_proba(X_te)[:,1]\n",
    "    n = len(y_te)\n",
    "    for topk in top_ks:\n",
    "        k = max(1, int(np.floor(topk * n)))\n",
    "        idx = np.argsort(scores)[-k:]\n",
    "        y_pred = np.zeros_like(y_te)\n",
    "        y_pred[idx] = 1\n",
    "        prec = precision_score(y_te, y_pred, zero_division=0)\n",
    "        rec  = recall_score(y_te, y_pred, zero_division=0)\n",
    "        results_topk.append({\n",
    "            \"window\": d,\n",
    "            \"top_K%\": int(topk*100),\n",
    "            \"precision\": prec,\n",
    "            \"recall\": rec\n",
    "        })\n",
    "\n",
    "df_topk = pd.DataFrame(results_topk)\n",
    "prec_table = df_topk.pivot(index=\"window\", columns=\"top_K%\", values=\"precision\").sort_index()\n",
    "rec_table  = df_topk.pivot(index=\"window\", columns=\"top_K%\", values=\"recall\").sort_index()\n",
    "print(\"=== Top-K Precision ===\")\n",
    "display(prec_table.style.format(\"{:.3f}\"))\n",
    "print(\"\\n=== Top-K Recall ===\")\n",
    "display(rec_table.style.format(\"{:.3f}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935dad5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Feature Importances (XGBoost)\n",
    "\n",
    "inspect_windows = [0, 3, 7]\n",
    "feature_importances = {}\n",
    "\n",
    "for d in inspect_windows:\n",
    "    X, y = window_datasets[d]\n",
    "    w = (y == 0).sum() / (y == 1).sum()\n",
    "    clf = XGBClassifier(\n",
    "        n_estimators=400,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=6,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        n_jobs=-1,\n",
    "        eval_metric=\"logloss\",\n",
    "        scale_pos_weight=w\n",
    "    )\n",
    "    pipe = make_pipe(clf, X)\n",
    "    pipe.fit(X, y)\n",
    "    ct = pipe.named_steps['prep']\n",
    "    num_cols = ct.transformers_[0][2]\n",
    "    cat_pipeline = ct.transformers_[1][1]\n",
    "    ohe = cat_pipeline.named_steps['ohe']\n",
    "    cat_cols = ct.transformers_[1][2]\n",
    "    try:\n",
    "        cat_names = ohe.get_feature_names(cat_cols)\n",
    "    except:\n",
    "        cat_names = ohe.get_feature_names_out(cat_cols)\n",
    "    feat_names = list(num_cols) + list(cat_names)\n",
    "    imps = pipe.named_steps['clf'].feature_importances_\n",
    "    top_idx = imps.argsort()[-5:][::-1]\n",
    "    feature_importances[d] = [feat_names[i] for i in top_idx]\n",
    "\n",
    "rows = []\n",
    "for d, feats in feature_importances.items():\n",
    "    for rank, feat in enumerate(feats, start=1):\n",
    "        rows.append({\"Window\": d, \"Rank\": rank, \"Feature\": feat})\n",
    "df_imp = pd.DataFrame(rows)\n",
    "pivot = df_imp.pivot(index=\"Rank\", columns=\"Window\", values=\"Feature\")\n",
    "display(pivot)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Paper6-new-env",
   "language": "python",
   "name": "paper6-new-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
