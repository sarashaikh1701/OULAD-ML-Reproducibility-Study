{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74dd0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# P10 Reproduction Script\n",
    "\"\"\"\n",
    "Title: \"A Withdrawal Prediction Model of At-Risk Learners\"\n",
    "Authors: David Tait, Stephen Lonn, Christopher Brooks\n",
    "Conference: Proceedings of the 9th International Learning Analytics & Knowledge Conference (LAK 2019)\n",
    "\n",
    "Purpose:\n",
    "End-to-end reproduction of the Paper 10 withdrawal prediction pipeline:\n",
    "    - Data loading and preprocessing from OULAD\n",
    "    - Feature engineering (demographics, VLE behaviour, assessments)\n",
    "    - Discretisation of numeric indicators\n",
    "    - Model training and evaluation across:\n",
    "        • Decision Tree (J48 equivalent)\n",
    "        • Random Forest\n",
    "        • TAN Bayesian Classifier\n",
    "        • SVM\n",
    "        • MLP\n",
    "    - Performance comparison across balanced/unbalanced datasets\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b55616",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1\n",
    "import os, random, numpy as np, pandas as pd\n",
    "os.environ[\"PYTHONHASHSEED\"] = \"1\"\n",
    "random.seed(1)\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54b0591",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 2\n",
    "student_info = pd.read_csv(\"studentInfo.csv\")\n",
    "student_reg = pd.read_csv(\"studentRegistration.csv\")\n",
    "student_assessment = pd.read_csv(\"studentAssessment.csv\")\n",
    "assessments = pd.read_csv(\"assessments.csv\")\n",
    "student_vle = pd.read_csv(\"studentVle.csv\")\n",
    "vle = pd.read_csv(\"vle.csv\")\n",
    "courses = pd.read_csv(\"courses.csv\")\n",
    "MOD, PRES = \"DDD\", \"2013B\"\n",
    "student_info_f = student_info.query(\"code_module==@MOD and code_presentation==@PRES\")\n",
    "student_reg_f = student_reg.query(\"code_module==@MOD and code_presentation==@PRES\")\n",
    "student_vle_f = student_vle.query(\"code_module==@MOD and code_presentation==@PRES\")\n",
    "vle_f = vle.query(\"code_module==@MOD and code_presentation==@PRES\")\n",
    "courses_f = courses.query(\"code_module==@MOD and code_presentation==@PRES\")\n",
    "assessments_f = assessments.query(\"code_module==@MOD and code_presentation==@PRES\")\n",
    "student_assessment_f = student_assessment.merge(\n",
    "    assessments_f[[\"id_assessment\",\"code_module\",\"code_presentation\"]],\n",
    "    on=\"id_assessment\", how=\"inner\"\n",
    ")\n",
    "df_master = student_info_f.merge(\n",
    "    student_reg_f,\n",
    "    on=[\"code_module\",\"code_presentation\",\"id_student\"], how=\"left\"\n",
    ").merge(\n",
    "    courses_f,\n",
    "    on=[\"code_module\",\"code_presentation\"], how=\"left\"\n",
    ")\n",
    "assert len(df_master)==1303\n",
    "print(\"STEP 2 OK:\", df_master.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f062b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 3\n",
    "age_map = {\"0-35\": \"young\", \"35-55\": \"middle\", \"55<=\": \"senior\"}\n",
    "df_demo = df_master[[\"id_student\", \"gender\", \"region\", \"highest_education\",\n",
    "                     \"num_of_prev_attempts\", \"disability\", \"age_band\"]] \\\n",
    "            .assign(age_group=lambda d: d[\"age_band\"].map(age_map)) \\\n",
    "            .drop(columns=\"age_band\").set_index(\"id_student\")\n",
    "df_vle_long = student_vle_f.merge(vle_f, on=[\"code_module\",\"code_presentation\",\"id_site\"], how=\"left\")\n",
    "autonomy = df_vle_long.groupby(\"id_student\")[\"id_site\"].nunique().rename(\"autonomy\")\n",
    "motivation = df_vle_long.groupby(\"id_student\")[\"sum_click\"].sum().rename(\"motivation\")\n",
    "bucket_map = {\n",
    "    \"collab\": {\"forumng\",\"ouwiki\",\"oucollaborate\",\"ouelluminate\"},\n",
    "    \"structure\": {\"homepage\",\"glossary\",\"dataplus\",\"oudictionary\",\"oucontentindex\",\"oucollaborate\"},\n",
    "    \"content\": {\"resource\",\"oucontent\",\"page\",\"subpage\",\"url\"},\n",
    "    \"evaluate\": {\"quiz\",\"questionnaire\",\"ouexam\",\"ouelluminatequiz\"}\n",
    "}\n",
    "def bucketize(act):\n",
    "    for label, toys in bucket_map.items():\n",
    "        if act in toys:\n",
    "            return label\n",
    "    return None\n",
    "df_vle_long[\"commit_bucket\"] = df_vle_long[\"activity_type\"].map(bucketize)\n",
    "commit_buckets = [\"collab\",\"structure\",\"content\",\"evaluate\"]\n",
    "commit = df_vle_long.pivot_table(\n",
    "    index=\"id_student\", columns=\"commit_bucket\", values=\"sum_click\",\n",
    "    aggfunc=\"sum\", fill_value=0\n",
    ").reindex(columns=commit_buckets, fill_value=0).rename(columns={\n",
    "    \"collab\":\"commit_collab\",\n",
    "    \"structure\":\"commit_structure\",\n",
    "    \"content\":\"commit_content\",\n",
    "    \"evaluate\":\"commit_evaluate\"\n",
    "})\n",
    "df_assess_long = student_assessment_f.merge(\n",
    "    assessments_f, on=[\"code_module\",\"code_presentation\",\"id_assessment\"], how=\"left\"\n",
    ")\n",
    "df_assess_long[\"weighted_score\"] = df_assess_long[\"score\"] * df_assess_long[\"weight\"]\n",
    "performance = df_assess_long.groupby(\"id_student\")[\"weighted_score\"].sum().rename(\"performance\")\n",
    "total_assessments = assessments_f[\"id_assessment\"].nunique()\n",
    "df_assess_long[\"on_time\"] = df_assess_long[\"date_submitted\"] <= df_assess_long[\"date\"]\n",
    "perseverance = df_assess_long.groupby(\"id_student\")[\"on_time\"].sum().div(total_assessments).rename(\"perseverance\")\n",
    "df_indicators = df_demo.join([autonomy, motivation, commit, performance, perseverance]).reset_index()\n",
    "num_cols = df_indicators.select_dtypes(include=\"number\").columns\n",
    "df_indicators[num_cols] = df_indicators[num_cols].fillna(0)\n",
    "print(\"df_indicators shape:\", df_indicators.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ac2851",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 4\n",
    "target = df_master[[\"id_student\", \"final_result\"]].set_index(\"id_student\") \\\n",
    "    .squeeze().map(lambda x: 0 if x == \"Withdrawn\" else 1).rename(\"target\")\n",
    "df_model = df_indicators.set_index(\"id_student\").join(target).reset_index()\n",
    "print(\"Target distribution:\\n\", df_model[\"target\"].value_counts(), \"\\n\")\n",
    "print(\"df_model shape:\", df_model.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a91366",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 5\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "X = df_model.drop(columns=[\"id_student\",\"target\"])\n",
    "y = df_model[\"target\"]\n",
    "cat_cols = [\"gender\", \"region\", \"highest_education\", \"age_group\", \"disability\"]\n",
    "num_cols = [c for c in X.columns if c not in cat_cols]\n",
    "preprocess = ColumnTransformer([\n",
    "    (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n",
    "    (\"num\", StandardScaler(), num_cols)\n",
    "])\n",
    "smote_pipe = Pipeline([(\"pre\", preprocess), (\"smote\", SMOTE(random_state=1))])\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "train_idx, test_idx = next(cv.split(X, y))\n",
    "X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n",
    "print(\"Before SMOTE:\", np.bincount(y_train))\n",
    "X_res, y_res = smote_pipe.fit_resample(X_train, y_train)\n",
    "print(\"After SMOTE:\", np.bincount(y_res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee8579a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 6\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "df_disc = df_model.copy()\n",
    "num_inds = [\"autonomy\", \"motivation\", \"commit_collab\", \"commit_structure\",\n",
    "            \"commit_content\", \"commit_evaluate\", \"performance\", \"perseverance\"]\n",
    "def discretise(series, rs=1, cutoff=0.15):\n",
    "    if series.nunique(dropna=False) <= 1:\n",
    "        return pd.Series([\"low\"] * len(series), index=series.index)\n",
    "    X = series.values.reshape(-1,1)\n",
    "    km2 = KMeans(n_clusters=2, random_state=rs).fit(X)\n",
    "    km3 = KMeans(n_clusters=3, random_state=rs).fit(X)\n",
    "    drop = (km2.inertia_ - km3.inertia_) / km2.inertia_\n",
    "    if drop >= cutoff:\n",
    "        labels, mapping = km3.labels_, {0:\"low\",1:\"medium\",2:\"high\"}\n",
    "    else:\n",
    "        labels, mapping = km2.labels_, {0:\"low\",1:\"high\"}\n",
    "    return pd.Series(labels, index=series.index).map(mapping)\n",
    "for col in num_inds:\n",
    "    df_disc[col] = discretise(df_disc[col])\n",
    "X_raw = df_disc.drop(columns=[\"id_student\",\"target\"])\n",
    "y = df_disc[\"target\"]\n",
    "enc = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)\n",
    "X_enc = enc.fit_transform(X_raw)\n",
    "print(\"df_disc shape:\", df_disc.shape)\n",
    "print(\"Encoded matrix shape:\", X_enc.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24332af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 7\n",
    "\n",
    "#Step 7.1\n",
    "df_vle_long = student_vle_f.merge(\n",
    "    vle_f,\n",
    "    on=[\"code_module\",\"code_presentation\",\"id_site\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "X_vle_raw = df_vle_long.pivot_table(\n",
    "    index=\"id_student\",\n",
    "    columns=\"activity_type\",\n",
    "    values=\"sum_click\",\n",
    "    aggfunc=\"sum\",\n",
    "    fill_value=0\n",
    ")\n",
    "demo_cols = [\n",
    "    \"gender\",\n",
    "    \"region\",\n",
    "    \"highest_education\",\n",
    "    \"age_band\",\n",
    "    \"num_of_prev_attempts\",\n",
    "    \"disability\"\n",
    "]\n",
    "X_demo = df_master.set_index(\"id_student\")[demo_cols]\n",
    "X_base = (\n",
    "    df_model[[\"id_student\"]]\n",
    "      .set_index(\"id_student\")\n",
    "      .join(X_demo)\n",
    "      .join(X_vle_raw)\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "prep_base = ColumnTransformer([\n",
    "    (\"cat\", SKPipeline([\n",
    "        (\"imp\", SimpleImputer(strategy=\"constant\", fill_value=\"missing\")),\n",
    "        (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "    ]), demo_cols),\n",
    "    (\"num\", SKPipeline([\n",
    "        (\"imp\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler())\n",
    "    ]), X_vle_raw.columns.tolist())\n",
    "])\n",
    "\n",
    "#Step 7.2\n",
    "X_cont = df_model.drop(columns=[\"id_student\",\"target\"])\n",
    "prep_cont = ColumnTransformer([\n",
    "    (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"),\n",
    "        [\"gender\",\"region\",\"highest_education\",\"age_group\",\"disability\"]),\n",
    "    (\"num\", StandardScaler(),\n",
    "        [\"autonomy\",\"motivation\",\n",
    "         \"commit_collab\",\"commit_structure\",\n",
    "         \"commit_content\",\"commit_evaluate\",\n",
    "         \"performance\",\"perseverance\"])\n",
    "])\n",
    "\n",
    "#Step 7.3\n",
    "X_disc = df_disc.drop(columns=[\"id_student\",\"target\"])\n",
    "prep_disc = ColumnTransformer([\n",
    "    (\"ord\", OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1),\n",
    "        X_disc.columns.tolist())\n",
    "])\n",
    "\n",
    "#Step 7.4\n",
    "y = df_model[\"target\"]\n",
    "\n",
    "#Step 7.5\n",
    "variants = [\n",
    "    (\"Unbal / No Ind\",   X_base,  prep_base, False),\n",
    "    (\"Unbal / Numeric\",  X_cont,  prep_cont, False),\n",
    "    (\"Unbal / Discrete\", X_disc,  prep_disc, False),\n",
    "    (\"Bal / No Ind\",     X_base,  prep_base, True),\n",
    "    (\"Bal / Numeric\",    X_cont,  prep_cont, True),\n",
    "    (\"Bal / Discrete\",   X_disc,  prep_disc, True),\n",
    "]\n",
    "\n",
    "#Step 7.6\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "dt = DecisionTreeClassifier(\n",
    "    random_state=1,\n",
    "    criterion=\"entropy\",\n",
    "    ccp_alpha=0.0\n",
    ")\n",
    "results = []\n",
    "for label, Xv, prep, balance in variants:\n",
    "    steps = [(\"pre\", prep)]\n",
    "    if balance:\n",
    "        steps.append((\"smote\", SMOTE(random_state=1)))\n",
    "    steps.append((\"clf\", dt))\n",
    "    pipe = Pipeline(steps)\n",
    "    f1 = cross_val_score(pipe, Xv, y, scoring=\"f1\", cv=cv, n_jobs=-1)\n",
    "    results.append((label, f1.mean().round(3)))\n",
    "df_dt = pd.DataFrame(results, columns=[\"Variant\",\"F1\"]).set_index(\"Variant\")\n",
    "df_dt = df_dt.style.set_caption(\"Decision trees\")\n",
    "df_dt\n",
    "\n",
    "#Step 7.7\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=1)\n",
    "results = []\n",
    "for label, Xv, prep, balance in variants:\n",
    "    steps = [(\"pre\", prep)]\n",
    "    if balance:\n",
    "        steps.append((\"smote\", SMOTE(random_state=1)))\n",
    "    steps.append((\"clf\", rf))\n",
    "    pipe = Pipeline(steps)\n",
    "    f1 = cross_val_score(pipe, Xv, y, scoring=\"f1\", cv=cv, n_jobs=-1)\n",
    "    results.append((label, f1.mean().round(3)))\n",
    "df_rf = pd.DataFrame(results, columns=[\"Variant\",\"F1\"]).set_index(\"Variant\")\n",
    "df_rf = df_rf.style.set_caption(\"Random Forest\")\n",
    "df_rf\n",
    "\n",
    "#Step 7.8\n",
    "def foldwise_binarize(df_train, df_test):\n",
    "    tr = df_train.copy()\n",
    "    te = df_test.copy()\n",
    "    for c in tr.columns:\n",
    "        if pd.api.types.is_numeric_dtype(tr[c]) and tr[c].nunique() > 2:\n",
    "            med = tr[c].median()\n",
    "            tr[c] = np.where(tr[c] <= med, \"low\", \"high\")\n",
    "            te[c] = np.where(te[c] <= med, \"low\", \"high\")\n",
    "        else:\n",
    "            tr[c] = tr[c].astype(str)\n",
    "            te[c] = te[c].astype(str)\n",
    "    return tr, te\n",
    "results = []\n",
    "for label, Xv, prep_unused, balance in variants:\n",
    "    f1s = []\n",
    "    for train_idx, test_idx in cv.split(Xv, y):\n",
    "        X_tr, X_te = Xv.iloc[train_idx], Xv.iloc[test_idx]\n",
    "        y_tr, y_te = y.iloc[train_idx], y.iloc[test_idx]\n",
    "        tr_bin, te_bin = foldwise_binarize(X_tr, X_te)\n",
    "        enc = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)\n",
    "        X_tr_enc = enc.fit_transform(tr_bin)\n",
    "        if balance:\n",
    "            X_tr_bal, y_tr_bal = SMOTE(random_state=1).fit_resample(X_tr_enc, y_tr.values)\n",
    "            tr_bal = pd.DataFrame(enc.inverse_transform(X_tr_bal), columns=tr_bin.columns)\n",
    "            y_bal = pd.Series(y_tr_bal)\n",
    "        else:\n",
    "            tr_bal, y_bal = tr_bin, y_tr\n",
    "        train_df = tr_bal.copy()\n",
    "        train_df[\"target\"] = y_bal.astype(str)\n",
    "        test_df = te_bin.copy()\n",
    "        test_df[\"target\"] = y_te.astype(str)\n",
    "        ts = TreeSearch(train_df)\n",
    "        dag = ts.estimate(estimator_type=\"tan\", class_node=\"target\")\n",
    "        bn = BayesianModel(dag.edges())\n",
    "        bn.fit(train_df, estimator=BayesianEstimator, prior_type=\"BDeu\")\n",
    "        y_pred = bn.predict(test_df.drop(columns=\"target\"))[\"target\"].astype(int)\n",
    "        f1s.append(f1_score(test_df[\"target\"].astype(int), y_pred))\n",
    "    results.append((label, np.mean(f1s).round(3)))\n",
    "df_tan = pd.DataFrame(results, columns=[\"Variant\",\"F1\"]).set_index(\"Variant\")\n",
    "df_tan = df_tan.style.set_caption(\"Bayesian Classifier (TAN)\")\n",
    "df_tan\n",
    "\n",
    "#Step 7.9\n",
    "f1w = make_scorer(f1_score, pos_label=0)\n",
    "svm = SVC(kernel=\"linear\", C=1, tol=0.1, random_state=1)\n",
    "dense = FunctionTransformer(\n",
    "    lambda X: X.A if hasattr(X, \"A\")\n",
    "              else (X.toarray() if hasattr(X, \"toarray\") else X),\n",
    "    accept_sparse=True\n",
    ")\n",
    "results = []\n",
    "for label, Xv, prep, balance in variants:\n",
    "    steps = [(\"pre\", prep), (\"dense\", dense), (\"norm\", MinMaxScaler())]\n",
    "    if balance:\n",
    "        steps.append((\"smote\", SMOTE(random_state=1)))\n",
    "    steps.append((\"clf\", svm))\n",
    "    f1 = cross_val_score(Pipeline(steps), Xv, y, scoring=f1w, cv=cv, n_jobs=-1)\n",
    "    results.append((label, np.round(f1.mean(), 3)))\n",
    "df_svm = pd.DataFrame(results, columns=[\"Variant\", \"F1\"]).set_index(\"Variant\")\n",
    "df_svm = df_svm.style.set_caption(\"Support Vector Machine (SVM)\")\n",
    "df_svm\n",
    "\n",
    "#Step 7.10\n",
    "mlp = MLPClassifier(\n",
    "    solver='sgd',\n",
    "    learning_rate_init=0.3,\n",
    "    momentum=0.2,\n",
    "    random_state=1\n",
    ")\n",
    "results = []\n",
    "for label, Xv, prep, balance in variants:\n",
    "    steps = [(\"pre\", prep)]\n",
    "    if balance:\n",
    "        steps.append((\"smote\", SMOTE(random_state=1)))\n",
    "    steps.append((\"clf\", mlp))\n",
    "    pipe = Pipeline(steps)\n",
    "    f1 = cross_val_score(pipe, Xv, y, scoring=\"f1\", cv=cv, n_jobs=-1)\n",
    "    results.append((label, round(f1.mean(), 3)))\n",
    "df_mlp = pd.DataFrame(results, columns=[\"Variant\",\"F1\"]).set_index(\"Variant\")\n",
    "df_mlp.style.set_caption(\"Artificial Neural Network (MLP)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17238454",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 8\n",
    "\n",
    "def _unwrap(styler_or_df):\n",
    "    return styler_or_df.data if hasattr(styler_or_df, \"data\") else styler_or_df\n",
    "\n",
    "frames = []\n",
    "for name, df_model in [\n",
    "        (\"Decision Tree\",  df_dt),\n",
    "        (\"Random Forest\",  df_rf),\n",
    "        (\"SVM\",            df_svm),\n",
    "        (\"MLP\",            df_mlp),\n",
    "        (\"Bayesian (TAN)\", df_tan)]:\n",
    "    tmp = _unwrap(df_model).reset_index()\n",
    "    tmp[\"Model\"] = name\n",
    "    frames.append(tmp[[\"Model\", \"Variant\", \"F1\"]])\n",
    "\n",
    "df_all = pd.concat(frames, ignore_index=True)\n",
    "\n",
    "order = [\"Unbal / No Ind\", \"Unbal / Numeric\", \"Unbal / Discrete\",\n",
    "         \"Bal / No Ind\",   \"Bal / Numeric\",   \"Bal / Discrete\"]\n",
    "\n",
    "(df_all.assign(F1=lambda d: d[\"F1\"].round(3))\n",
    "      .pivot(index=\"Model\", columns=\"Variant\", values=\"F1\")\n",
    "      .reindex(columns=order)\n",
    "      .style.set_caption(\"F1 scores\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
